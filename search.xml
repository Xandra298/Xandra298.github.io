<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Learning Multimodal Violence Detection under Weak Supervision</title>
    <url>/posts/undefined/</url>
    <content><![CDATA[<h1 id="Not-only-look-but-also-listen-Learning-multimodal-violence-detection-under-weak-supervision"><a href="#Not-only-look-but-also-listen-Learning-multimodal-violence-detection-under-weak-supervision" class="headerlink" title="Not only look, but also listen: Learning multimodal violence detection under weak supervision"></a>Not only look, but also listen: Learning multimodal violence detection under weak supervision</h1><p><a href="https://arxiv.org/pdf/2007.04687.pdf">paper</a> ，<a href="https://roc-ng.github.io/XD-Violence/">code and dataset</a></p>
<p>暴力视频检测方向论文</p>
<blockquote>
<p> 💡 We introduce a HL-Net to simultaneously capture <strong>long-range relations</strong> and<br><strong>local distance relations</strong>, of which these two relations are based on similarity prior and proximity prior, respectively</p>
<p>三个并行的branch捕捉视频片段和集成的特征之间的不同联系:</p>
<ul>
<li>holistic branch captures long-range dependencies using similarity prior,</li>
<li>localized branch captures local positional relation using proximity prior,</li>
<li>score branch dynamically captures the closeness of predicted score.</li>
</ul>
</blockquote>
<span id="more"></span>
<p>相关工作，其中attention部分可以后面看下；</p>
<p>一些工作将图神经网络(GCNs)[20,39]在图上建立不同节点之间的关系模型，并学习计算机视觉的强大表示。例如，GCN被用于时间性动作定位[50]、视频分类[37,41]、异常检测[51]。基于骨架的动作识别[33,45]，点云语义分割[21]，图像说明[46]等等。除了GCN，时间关系网络[52]，旨在学习和推理视频帧之间的时间依赖关系，被提出来用于解决视频分类。最近，自我注意网络[40,47,5,18]已被成功应用于视觉问题。注意力操作可以通过聚合一组元素的信息来影响单个元素，其中聚合的权重是自动学习的。</p>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><h2 id="总体框架图"><a href="#总体框架图" class="headerlink" title="总体框架图"></a>总体框架图</h2><p><img src="/posts/undefined/Untitled.png" alt></p>
<ul>
<li><p>特征提取：using the sliding window mechanism 提取 视频特征和声音特征，合并成融合特征（融合部分不予关注。滑动窗口机制待看）</p>
<blockquote>
<p>Visual features: utilize two mainstream networks-C3D and I3D networks. we extract fc6 features from C3D that is pretrained on the Sports-1M dataset, and extract global_pool features from I3D pre-trained on Kinetics-200 dataset.</p>
</blockquote>
</li>
</ul>
<h2 id="Holistic-and-Localized-Networks"><a href="#Holistic-and-Localized-Networks" class="headerlink" title="Holistic and Localized Networks"></a>Holistic and Localized Networks</h2><h3 id="Holistic-特征相似性"><a href="#Holistic-特征相似性" class="headerlink" title="Holistic - 特征相似性"></a><strong>Holistic - 特征相似性</strong></h3><hr>
<p>通用的图卷积表示可以看为：</p>
<script type="math/tex; mode=display">
X_l+1 = Update(Aggregate(X_l,W_l^{agg},W_l^{update}))</script><ul>
<li><strong>GCN范式：</strong><script type="math/tex; mode=display">
X^H_{l+1} = Dropout(ReLU(A^HX^H_lW^H_l))</script></li>
</ul>
<p>注：$X^H_0 = X^S_0 = X^L_0 = X^F$</p>
<p><strong>特征相似性的邻接矩阵表示:</strong></p>
<script type="math/tex; mode=display">
A^H_{ij} = g(f(x_i,x_j))</script><p>$A^H_{ij}$衡量第i个和第j个特征的特征相似性；g是归一化函数，f函数计算一对特征的相似性；附录部分还讨论了一下其他版本的。本文定义如下</p>
<ul>
<li>f 定义为：</li>
</ul>
<script type="math/tex; mode=display">
f(x_i,x_j) = \frac{x_i^Tx_j}{||x_i||_{2} \\ . \\ ||x_j||_2}</script><ul>
<li>thresholding操作（$\tau$是threshod，f将相似性限制在（0,1]之间）：</li>
</ul>
<script type="math/tex; mode=display">
f(x_i,x_j) = \begin{cases} f(x_i,x_j)  & f(x_i,x_j) > \tau \\ 0 & f(x_i,x_j)\leq \tau\end{cases}</script><ul>
<li><p>使用softmax作为归一化函数g，使得A的每一行的和都为1。</p>
<script type="math/tex; mode=display">
A^H_{ij} = \frac{exp(A^H_{ij})}{\sum ^{T'}_{k=1} exp(A^H_{ij})}</script></li>
</ul>
<h3 id="localized-branch-proximity-prior-和时间一致性类似）"><a href="#localized-branch-proximity-prior-和时间一致性类似）" class="headerlink" title="localized branch -proximity prior (和时间一致性类似）"></a><strong>localized branch -proximity prior (和时间一致性类似）</strong></h3><hr>
<script type="math/tex; mode=display">
A^L_{ij} = exp(\frac{-|i-j|^r}{\sigma })</script><h2 id="Online-detection-amp-score-branch"><a href="#Online-detection-amp-score-branch" class="headerlink" title="Online detection &amp; score branch"></a>Online detection &amp; score branch</h2><p>正如我们提到的，暴力检测系统不仅适用于离线检测（互联网录像机），也适用于在线检测（监控系统）。然而，<strong>上述HL-Net</strong>的在线检测受到了一个主要障碍的阻碍：<strong>HL-Net需要整个视频来获得长距离的依赖关系</strong>。为了跳出这个困境，我们提出了一个HLC近似器，只把以前的视频片段作为输入，在HL-Net的指导下产生精确的预测。两个堆叠的FC层和ReLU以及一个一维因果卷积层构成了HLC近似器。一维因果卷积层的核大小为5，跨度为1，在时间上滑动卷积滤波器。一维因果卷积层也充当分类器，其输出是形状为 T’ 的暴力激活表示为$C^S$。更妙的是，这个操作引入了一个额外的分支，名为<strong>动态得分分支(dynamic score branch)</strong>，以扩展HL-Net，它取决于$C^S$。</p>
<h3 id="score-branch"><a href="#score-branch" class="headerlink" title="score branch"></a>score branch</h3><p>用于online detection, 解决需要将整个视频作为输入（以获得长距离依赖）的问题。</p>
<p>该分支的主要作用是将一个位置的响应计算为所有位置特征的加权和，其中权重取决于分数的接近程度。与整体和局部分支的关系矩阵不同，分数分支的关系矩阵在每次迭代中都会更新，并且取决于预测的分数而不是先验。从形式上看，分数分支的关系矩阵设计如下：</p>
<script type="math/tex; mode=display">
A^S_{ij} = \rho(1 - |s(C^S_i) - s(C^S_j)|)</script><p>s是 sigmoid, 函数ρ用于加强（和削弱）得分接近度大于（和小于）0.5的配对关系，softmax也用于归一化。</p>
<script type="math/tex; mode=display">
\rho (x) = \frac{1}{1+exp(-\frac{x-0.5}{0.1})}</script><h2 id="training-based-on-MIL"><a href="#training-based-on-MIL" class="headerlink" title="training based on MIL"></a>training based on MIL</h2><script type="math/tex; mode=display">
C^P = (X^H||X^L||X^S)W</script><p>所有输出($C^P 和 C^S$)在时间维度上的K-max取平均作为输出，以得到$y^P$ 和$y^S$。K定义为T‘ 除以q加一后向下取整。</p>
<p>补充： top-k策略（Weakly-supervised video anomaly detection with robust temporal feature magnitude learning）</p>
<ul>
<li>loss结合三个branch</li>
</ul>
<blockquote>
<p>The instances corresponding to the K-max activation in the positive bag is most likely to be true positive instances (violence). The instances corresponding to the K-max activation in the negative bag is hard instances. We expect these two types of instances to be as far as possible.</p>
</blockquote>
<p>L_BCE（binary crossentropy) 和L_BCE2分别对应为$y^p 和 y^s$ 与 ground truth y之间的loss。L_DISTILL为知识蒸馏损失。</p>
<script type="math/tex; mode=display">
L_{TOTAL}  = L_{BCE} + L_{BCE2} +\lambda L_{DISTILL}</script><script type="math/tex; mode=display">
L_{DISTILL} = \sum \limits^N_{j=1} (- \sum \limits_i s(C^P_i) log (s(C_i^S)))</script><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>方法支持线上和离线的检测。sigmoid函数作为$C^P$和$C^S$的激活函数，并最后生成在[0,1]之间的暴力置信得分。注：在线上预测中，只有HLC近似器工作，HL-NET可以移除。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="评估标准"><a href="#评估标准" class="headerlink" title="评估标准"></a>评估标准</h2><p>we utilize the frame-level <strong>precision-recall curve</strong> (PRC) and corresponding area under the curve <strong>(average precision, AP)</strong> [30] rather than receiver operating characteristic curve (ROC) and corresponding area under the curve (AUC) [44,43]<br><strong>since AUC usually shows an optimistic result when dealing with class-imbalanced data, and PRC and AP focus on positive samples (violence)</strong></p>
<ul>
<li>Precision and Recall (PR曲线)：用于稀有事件检测，如目标检测、信息检索、推荐系统。负样本很多的时候，??? = FP⁄(FP+TN）很小，比较TPR和FPR没有太大意义（ROC）</li>
</ul>
<h2 id="性能表现"><a href="#性能表现" class="headerlink" title="性能表现"></a>性能表现</h2><p><img src="/posts/undefined/Untitled 1.png" alt></p>
<p>优于当前最先进的方法（20年的文章）</p>
<p>我们观察到，在我们的暴力检测任务中，C3D比I3D差了很大一截。</p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><ul>
<li>五种模态AP对比</li>
</ul>
<p><img src="/posts/undefined/Untitled 2.png" alt="Untitled"></p>
<p>（该文证明声音和视觉的融合检测效果更好，且视觉模态的作用优于声音）</p>
<ul>
<li><p>三个分支的对比（holistic, localized and score branches）</p>
<ul>
<li>三个分支单独的情况表现相似</li>
<li>移除任何一个分支都会使得表现变差</li>
<li>HL-NET在这三个分支一起作用的时候表现最好，因此证明三个分支都不可替代。</li>
</ul>
<p><img src="/posts/undefined/Untitled 3.png" alt></p>
</li>
<li><p>online vs offline</p>
<p><img src="/posts/undefined/Untitled 4.png" alt></p>
</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h3 id="光流"><a href="#光流" class="headerlink" title="光流"></a>光流</h3><blockquote>
<p>光流（optical flow）是空间运动物体在观察成像平面上的像素运动的瞬时速度。<br>光流法是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。<br>通常将二维图像平面特定坐标点上的灰度瞬时变化率定义为光流矢量。</p>
</blockquote>
<p>作者在《On the Integration of Optical Flow and Action Recognition》这篇文章[1]中深入讨论了光流与行为识别的结合，并通过实验观察到如下结论：<br>（1）光流对于行为识别是有用的，因为它的外观不变性；<br>（2）光流法采用最小化端点误差（EPE，end-point-error）来优化，但是当前EPE方法与动作识别性能没有很好的相关性；<br>（3）对于测试过的光流方法，在边界上和小位移上的精度与动作识别性能最相关；<br>（4）采用最小化分类误差（而非EPE）来训练光流可以提高识别性能；<br>（5）用于行为识别任务的光流不同于传统的光流，特别是在人体内部和身体边界处。<br>原文链接：<a href="https://blog.csdn.net/zhang_can/article/details/80259946">https://blog.csdn.net/zhang_can/article/details/80259946</a></p>
]]></content>
      <categories>
        <category>视频检测</category>
      </categories>
      <tags>
        <tag>深度视频检测</tag>
        <tag>暴力视频检测</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo Hello World</title>
    <url>/posts/1e44dbaf/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>hexo 文档</category>
      </categories>
  </entry>
  <entry>
    <title>pytorchlearning</title>
    <url>/posts/643f7c73/</url>
    <content><![CDATA[<h1 id="pytorch-学习记录"><a href="#pytorch-学习记录" class="headerlink" title="pytorch 学习记录"></a>pytorch 学习记录</h1><p>待更…</p>
<p>视频学习笔记github存储仓库：（@ljh）</p>
<p><a href="https://github.com/Xandra298/Pytorchlearning">Xandra298/Pytorchlearning (github.com)</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>第一次搭建hexo博客</title>
    <url>/posts/83682157/</url>
    <content><![CDATA[<h2 id="hexo-deploy-到github失败"><a href="#hexo-deploy-到github失败" class="headerlink" title="hexo deploy 到github失败"></a>hexo deploy 到github失败</h2><p>使用deploy之后要求输入账户和密码，密码正确但是显示验证失败。</p>
<span id="more"></span>
<ul>
<li><p>失败原因：</p>
<p>github 21年取消了密码验证</p>
<p><img src="/posts/83682157/image-20221117162546351.png" alt="image-20221117162546351"></p>
</li>
<li><p>解决方式</p>
<p>在_config.yml的deploy中，repo对应的填写为github仓库的ssh地址，可以避免进行密码验证（前提：git配置完成，github上有设置公钥）</p>
</li>
</ul>
<h2 id="完成的配置"><a href="#完成的配置" class="headerlink" title="完成的配置"></a>完成的配置</h2><ul>
<li><p>使用hexo主题</p>
<p><a href="https://github.com/probberechts/hexo-theme-cactus">hexo-theme-cactus</a></p>
<ul>
<li><a href="https://github.com/theme-next/hexo-theme-next">theme-next/hexo-theme-next: Elegant and powerful theme for Hexo. (github.com)</a></li>
</ul>
</li>
<li><p>主题配置 tags 和 categories</p>
</li>
<li><p>配置baidu_analytics</p>
</li>
<li><p>comment</p>
<ul>
<li>使用utterances设置comment</li>
<li>来必力</li>
</ul>
</li>
<li><p>不蒜子统计</p>
</li>
<li><p>使用主题的search</p>
<ul>
<li>不成功-&gt;解决：发现是npm install的时候没有在博客根目录导致的问题</li>
</ul>
</li>
<li><p>解决图片路径问题</p>
<ul>
<li><p>sources/images目录下存引图片</p>
</li>
<li><p>setup那篇post使用post_asset_folder: true的方式（需要安装hexo插件），将资源放在对应目录下</p>
<ul>
<li><p>本地成功但是服务器显示无效，重新安装插件成功：</p>
<p><code>npm install https://github.com/7ym0n/hexo-asset-image --save</code></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>hexo new post失败</p>
<ul>
<li>使用 hexo new ‘[post]’ “postname”</li>
</ul>
</li>
<li><p>数学公式渲染</p>
<ul>
<li><a href="https://blog.csdn.net/qq_38496329/article/details/104065659">https://blog.csdn.net/qq_38496329/article/details/104065659</a></li>
<li>注意在文章中front 部分设置mathjax: true</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>记录</tag>
      </tags>
  </entry>
  <entry>
    <title>邻接矩阵表示成PyG需要的edge_index并进行带边权的网络构建</title>
    <url>/posts/35a1541/</url>
    <content><![CDATA[<h1 id="邻接矩阵-to-pyg需要的edge-index格式"><a href="#邻接矩阵-to-pyg需要的edge-index格式" class="headerlink" title="邻接矩阵 to pyg需要的edge_index格式"></a>邻接矩阵 to pyg需要的edge_index格式</h1><span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.rand([<span class="number">10</span>,<span class="number">10</span>]) <span class="comment"># 10*10的邻接矩阵A，带有权值，而非0/1</span></span><br><span class="line">A</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[0.8253, 0.2458, 0.9340, 0.4631, 0.5114, 0.3248, 0.8528, 0.6354, 0.2988,
         0.1087],
        [0.0190, 0.5693, 0.4843, 0.9588, 0.6011, 0.5755, 0.4621, 0.7694, 0.0637,
         0.9790],
        [0.6978, 0.9686, 0.9701, 0.2234, 0.5633, 0.9978, 0.9766, 0.3365, 0.3512,
         0.2396],
        [0.3582, 0.9965, 0.7739, 0.5641, 0.7275, 0.3078, 0.1826, 0.5449, 0.6566,
         0.1949],
        [0.8194, 0.7996, 0.9177, 0.3419, 0.5239, 0.7048, 0.4503, 0.0758, 0.2244,
         0.0659],
        [0.6131, 0.3546, 0.0789, 0.2735, 0.0781, 0.8000, 0.0587, 0.6644, 0.2678,
         0.6351],
        [0.7244, 0.0463, 0.9280, 0.6456, 0.6837, 0.0763, 0.0759, 0.0440, 0.1849,
         0.8942],
        [0.3589, 0.6925, 0.2334, 0.3476, 0.6695, 0.1048, 0.1470, 0.5548, 0.4736,
         0.6934],
        [0.0356, 0.8016, 0.6176, 0.2867, 0.1340, 0.7196, 0.0562, 0.5548, 0.7376,
         0.2841],
        [0.9301, 0.1725, 0.4012, 0.3893, 0.8366, 0.1587, 0.3342, 0.7945, 0.8123,
         0.8724]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line">adj  = sp.coo_matrix(A) <span class="comment">#转换成coo_matrix矩阵</span></span><br><span class="line">values = adj.data</span><br><span class="line">values,adj.row,adj.col</span><br></pre></td></tr></table></figure>
<pre><code>(array([0.825252  , 0.24581629, 0.9340454 , 0.4631123 , 0.51142365,
        0.32479858, 0.85282457, 0.63537604, 0.29877287, 0.10873711,
        0.01895636, 0.5693259 , 0.48427123, 0.9587981 , 0.6010562 ,
        0.57548887, 0.46208388, 0.7693816 , 0.06371653, 0.97895676,
        0.6978117 , 0.9685761 , 0.97011906, 0.22341514, 0.56326205,
        0.9978037 , 0.97661865, 0.33654213, 0.35123014, 0.23959029,
        0.358207  , 0.99651885, 0.7739324 , 0.5641022 , 0.72754997,
        0.3077591 , 0.18257308, 0.5449101 , 0.65663534, 0.1949212 ,
        0.8193548 , 0.79964596, 0.9176568 , 0.34189552, 0.5239384 ,
        0.70477635, 0.4503097 , 0.07584941, 0.22442049, 0.06589556,
        0.6130815 , 0.35458   , 0.07890564, 0.27350843, 0.07805085,
        0.79995   , 0.05868119, 0.66441715, 0.267847  , 0.6351336 ,
        0.72437716, 0.04632962, 0.92803836, 0.645646  , 0.6836786 ,
        0.07632524, 0.07594979, 0.04397732, 0.18492383, 0.89419115,
        0.3588807 , 0.6925135 , 0.23337674, 0.34763372, 0.66951907,
        0.10478634, 0.14702266, 0.55476344, 0.47362745, 0.69343317,
        0.03562325, 0.80160064, 0.6175768 , 0.2867241 , 0.13401723,
        0.719559  , 0.05618161, 0.55481714, 0.7375902 , 0.28414857,
        0.9300911 , 0.17248052, 0.4012187 , 0.38931435, 0.83664143,
        0.15867668, 0.3341686 , 0.7945494 , 0.81226593, 0.8724434 ],
       dtype=float32),
 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], dtype=int32),
 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
        2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,
        4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,
        6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,
        8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">indices = np.vstack((adj.row,adj.col)) <span class="comment"># 我们需要的coo形式的edge_index</span></span><br><span class="line">edge_index = torch.LongTensor(indices)<span class="comment">#PyG需要的edge_index</span></span><br><span class="line">edge_index,edge_index.shape</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,
          2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,
          4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,
          7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,
          9, 9, 9, 9],
         [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,
          4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,
          8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
          2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,
          6, 7, 8, 9]]),
 torch.Size([2, 100]))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_attr = adj.data <span class="comment">#边权值</span></span><br><span class="line">edge_attr = torch.FloatTensor(edge_attr)<span class="comment">#to float tensor</span></span><br><span class="line">edge_attr,edge_attr.shape</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([0.8253, 0.2458, 0.9340, 0.4631, 0.5114, 0.3248, 0.8528, 0.6354, 0.2988,
         0.1087, 0.0190, 0.5693, 0.4843, 0.9588, 0.6011, 0.5755, 0.4621, 0.7694,
         0.0637, 0.9790, 0.6978, 0.9686, 0.9701, 0.2234, 0.5633, 0.9978, 0.9766,
         0.3365, 0.3512, 0.2396, 0.3582, 0.9965, 0.7739, 0.5641, 0.7275, 0.3078,
         0.1826, 0.5449, 0.6566, 0.1949, 0.8194, 0.7996, 0.9177, 0.3419, 0.5239,
         0.7048, 0.4503, 0.0758, 0.2244, 0.0659, 0.6131, 0.3546, 0.0789, 0.2735,
         0.0781, 0.8000, 0.0587, 0.6644, 0.2678, 0.6351, 0.7244, 0.0463, 0.9280,
         0.6456, 0.6837, 0.0763, 0.0759, 0.0440, 0.1849, 0.8942, 0.3589, 0.6925,
         0.2334, 0.3476, 0.6695, 0.1048, 0.1470, 0.5548, 0.4736, 0.6934, 0.0356,
         0.8016, 0.6176, 0.2867, 0.1340, 0.7196, 0.0562, 0.5548, 0.7376, 0.2841,
         0.9301, 0.1725, 0.4012, 0.3893, 0.8366, 0.1587, 0.3342, 0.7945, 0.8123,
         0.8724]),
 torch.Size([100]))
</code></pre><h1 id="edge-index-to-邻接矩阵"><a href="#edge-index-to-邻接矩阵" class="headerlink" title="edge_index to 邻接矩阵"></a>edge_index to 邻接矩阵</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> sparse_coo_tensor</span><br><span class="line">adj = sparse_coo_tensor(edge_index,edge_attr,[<span class="number">10</span>,<span class="number">10</span>])</span><br><span class="line">adj.to_dense()</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[4.5977e-01, 6.6455e-01, 4.3946e-01, 3.8642e-01, 1.2331e-01, 2.9945e-01,
         2.5433e-01, 9.7476e-01, 4.5961e-04, 5.9594e-02],
        [2.2455e-01, 9.7698e-01, 8.7531e-01, 2.8142e-01, 7.0980e-01, 6.2595e-01,
         2.3625e-01, 5.7737e-01, 4.4227e-01, 6.5420e-01],
        [5.4512e-01, 2.4614e-01, 6.9270e-01, 6.8005e-01, 1.3384e-01, 5.9974e-01,
         9.2275e-01, 3.6578e-01, 3.5667e-01, 5.8081e-01],
        [9.6142e-02, 8.5471e-01, 5.9899e-02, 3.0163e-01, 2.9641e-01, 2.8706e-01,
         4.8757e-01, 8.8466e-01, 3.4357e-01, 9.9034e-01],
        [4.5909e-01, 7.2475e-01, 2.4294e-01, 7.3560e-01, 3.2247e-01, 7.6749e-01,
         3.6008e-01, 3.0816e-01, 7.4665e-01, 6.7713e-01],
        [6.6836e-01, 8.9111e-01, 8.0428e-01, 7.9984e-01, 6.5296e-01, 8.1743e-01,
         8.8702e-01, 3.6678e-01, 4.2774e-01, 2.3170e-02],
        [8.1350e-01, 1.6834e-01, 7.7933e-02, 3.8021e-01, 9.7750e-01, 5.6143e-01,
         7.9341e-01, 3.7514e-01, 9.3114e-01, 5.6821e-01],
        [8.4002e-01, 9.2273e-01, 5.6649e-01, 7.5386e-01, 9.1587e-01, 3.9596e-02,
         8.9435e-01, 5.6476e-01, 2.3289e-01, 1.9653e-01],
        [2.1682e-01, 2.8950e-01, 7.5310e-01, 6.7648e-01, 5.1057e-02, 1.6519e-01,
         5.8807e-01, 9.4542e-02, 6.3111e-01, 2.9049e-01],
        [5.7742e-02, 3.1503e-01, 5.6936e-01, 2.2748e-01, 4.8668e-01, 6.4949e-01,
         6.1752e-01, 3.9269e-01, 2.7897e-01, 5.5806e-01]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj = sp.coo_matrix((edge_attr,(edge_index[<span class="number">0</span>],edge_index[<span class="number">1</span>])),shape=[<span class="number">10</span>,<span class="number">10</span>])</span><br><span class="line">adj.toarray()</span><br></pre></td></tr></table></figure>
<pre><code>array([[4.5977378e-01, 6.6455245e-01, 4.3945801e-01, 3.8642406e-01,
        1.2331247e-01, 2.9944807e-01, 2.5433010e-01, 9.7475851e-01,
        4.5961142e-04, 5.9593856e-02],
       [2.2454953e-01, 9.7697508e-01, 8.7531334e-01, 2.8141612e-01,
        7.0980257e-01, 6.2595367e-01, 2.3624879e-01, 5.7737088e-01,
        4.4226754e-01, 6.5420014e-01],
       [5.4512197e-01, 2.4613553e-01, 6.9269532e-01, 6.8004644e-01,
        1.3383734e-01, 5.9973723e-01, 9.2274553e-01, 3.6578351e-01,
        3.5666680e-01, 5.8080733e-01],
       [9.6142113e-02, 8.5471165e-01, 5.9899449e-02, 3.0162632e-01,
        2.9641372e-01, 2.8705674e-01, 4.8757398e-01, 8.8466209e-01,
        3.4356719e-01, 9.9034435e-01],
       [4.5909441e-01, 7.2474545e-01, 2.4293584e-01, 7.3560286e-01,
        3.2246715e-01, 7.6749289e-01, 3.6007798e-01, 3.0815858e-01,
        7.4665487e-01, 6.7713338e-01],
       [6.6836429e-01, 8.9111018e-01, 8.0427557e-01, 7.9984426e-01,
        6.5295666e-01, 8.1743485e-01, 8.8702154e-01, 3.6678237e-01,
        4.2774427e-01, 2.3170471e-02],
       [8.1350172e-01, 1.6834372e-01, 7.7932715e-02, 3.8021082e-01,
        9.7749555e-01, 5.6143039e-01, 7.9341477e-01, 3.7514049e-01,
        9.3114382e-01, 5.6820768e-01],
       [8.4002483e-01, 9.2273450e-01, 5.6649190e-01, 7.5385606e-01,
        9.1587120e-01, 3.9596200e-02, 8.9435184e-01, 5.6475997e-01,
        2.3288828e-01, 1.9652534e-01],
       [2.1682233e-01, 2.8950059e-01, 7.5310403e-01, 6.7648250e-01,
        5.1056564e-02, 1.6518539e-01, 5.8806950e-01, 9.4541669e-02,
        6.3110876e-01, 2.9048622e-01],
       [5.7742238e-02, 3.1502587e-01, 5.6935811e-01, 2.2748303e-01,
        4.8667991e-01, 6.4949030e-01, 6.1752105e-01, 3.9268762e-01,
        2.7897447e-01, 5.5806071e-01]], dtype=float32)
</code></pre><h1 id="构建自定义边权重的GNN"><a href="#构建自定义边权重的GNN" class="headerlink" title="构建自定义边权重的GNN"></a>构建自定义边权重的GNN</h1><p>胡乱定值的<br>代码修改自：<a href="https://zhuanlan.zhihu.com/p/426907570">https://zhuanlan.zhihu.com/p/426907570</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(data.num_features, <span class="number">16</span>, cached=<span class="literal">True</span>,</span><br><span class="line">                             normalize=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#         self.conv1 = GATConv(data.num_features, 16)</span></span><br><span class="line">        <span class="comment">#self.conv2 = GCNConv(16, data.num_classes, cached=True,</span></span><br><span class="line">        self.conv2 = GCNConv(<span class="number">16</span>, <span class="number">2</span>, cached=<span class="literal">True</span>,</span><br><span class="line">                            normalize=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#         self.conv2 = GATConv(16, 2)</span></span><br><span class="line">        <span class="comment"># self.conv1 = ChebConv(data.num_features, 16, K=2)</span></span><br><span class="line">        <span class="comment"># self.conv2 = ChebConv(16, data.num_features, K=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr</span><br><span class="line">        x = F.relu(self.conv1(x, edge_index, edge_weight))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.conv2(x, edge_index, edge_weight)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv, ChebConv  <span class="comment"># noqa</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> data <span class="keyword">as</span> D</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>],[<span class="number">7</span>],[<span class="number">8</span>],[<span class="number">9</span>],[<span class="number">10</span>]], dtype=torch.<span class="built_in">float</span>)   <span class="comment"># N x emb(in)</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">x = torch.ones(<span class="number">10</span>,dtype=torch.<span class="built_in">float</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line"><span class="comment">#print(x.shape)</span></span><br><span class="line">y = torch.randint(<span class="number">0</span>,<span class="number">2</span>,[<span class="number">10</span>])</span><br><span class="line">train_mask = torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>], dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">val_mask=train_mask</span><br><span class="line">test_mask=train_mask</span><br><span class="line">data=D.Data()</span><br><span class="line">data.x,data.y,data.edge_index,data.edge_attr,data.train_mask,data.val_mask,data.test_mask \</span><br><span class="line">    = x,y,edge_index,edge_attr,train_mask,val_mask,test_mask</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([10, 1])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model, data = Net().to(device), data.to(device)</span><br><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">    <span class="built_in">dict</span>(params=model.conv1.parameters(), weight_decay=<span class="number">5e-4</span>),</span><br><span class="line">    <span class="built_in">dict</span>(params=model.conv2.parameters(), weight_decay=<span class="number">0</span>)</span><br><span class="line">], lr=<span class="number">0.01</span>)  <span class="comment"># Only perform weight-decay on first convolution.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()</span><br><span class="line">    <span class="comment">#F.nll_loss(model()[data], data.y).backward() #不行！</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    logits, accs = model(), []</span><br><span class="line">    <span class="keyword">for</span> _, mask <span class="keyword">in</span> data(<span class="string">&#x27;train_mask&#x27;</span>, <span class="string">&#x27;val_mask&#x27;</span>, <span class="string">&#x27;test_mask&#x27;</span>):</span><br><span class="line">        pred = logits[mask].<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        acc = pred.eq(data.y[mask]).<span class="built_in">sum</span>().item() / mask.<span class="built_in">sum</span>().item()</span><br><span class="line">        accs.append(acc)</span><br><span class="line">    <span class="keyword">return</span> accs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_acc = test_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    train()</span><br><span class="line">    train_acc, val_acc, tmp_test_acc = test()</span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; best_val_acc:</span><br><span class="line">        best_val_acc = val_acc</span><br><span class="line">        test_acc = tmp_test_acc</span><br><span class="line">    log = <span class="string">&#x27;Epoch: &#123;:03d&#125;, Train: &#123;:.4f&#125;, Val: &#123;:.4f&#125;, Test: &#123;:.4f&#125;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(log.<span class="built_in">format</span>(epoch, train_acc, best_val_acc, test_acc))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 001, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 002, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 003, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 004, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 005, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 006, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 007, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 008, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 009, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 010, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 011, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 012, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 013, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 014, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 015, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 016, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 017, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 018, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 019, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 020, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 021, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 022, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 023, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 024, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 025, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 026, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 027, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 028, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 029, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 030, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 031, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 032, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 033, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 034, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 035, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 036, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 037, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 038, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 039, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 040, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 041, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 042, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 043, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 044, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 045, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 046, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 047, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 048, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 049, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 050, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 051, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 052, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 053, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 054, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 055, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 056, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 057, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 058, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 059, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 060, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 061, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 062, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 063, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 064, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 065, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 066, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 067, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 068, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 069, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 070, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 071, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 072, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 073, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 074, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 075, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 076, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 077, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 078, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 079, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 080, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 081, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 082, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 083, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 084, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 085, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 086, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 087, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 088, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 089, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 090, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 091, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 092, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 093, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 094, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 095, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 096, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 097, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 098, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 099, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 100, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 101, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 102, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 103, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 104, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 105, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 106, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 107, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 108, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 109, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 110, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 111, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 112, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 113, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 114, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 115, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 116, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 117, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 118, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 119, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 120, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 121, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 122, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 123, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 124, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 125, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 126, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 127, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 128, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 129, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 130, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 131, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 132, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 133, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 134, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 135, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 136, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 137, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 138, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 139, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 140, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 141, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 142, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 143, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 144, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 145, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 146, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 147, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 148, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 149, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 150, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 151, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 152, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 153, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 154, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 155, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 156, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 157, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 158, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 159, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 160, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 161, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 162, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 163, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 164, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 165, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 166, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 167, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 168, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 169, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 170, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 171, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 172, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 173, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 174, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 175, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 176, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 177, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 178, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 179, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 180, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 181, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 182, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 183, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 184, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 185, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 186, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 187, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 188, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 189, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 190, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 191, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 192, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 193, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 194, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 195, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 196, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 197, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 198, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 199, Train: 0.6000, Val: 0.6000, Test: 0.6000
Epoch: 200, Train: 0.6000, Val: 0.6000, Test: 0.6000
</code></pre><h1 id="批量"><a href="#批量" class="headerlink" title="批量"></a>批量</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layer = GATConv(in_channels=<span class="number">16</span>, out_channels=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x,edg_index</span>):</span><br><span class="line">        output = torch.stack([self.layer(graph, edge_index=edge_indexi) <span class="keyword">for</span> graph,edge_indexi <span class="keyword">in</span> <span class="built_in">zip</span>(x,edg_index)], dim=<span class="number">0</span>)</span><br><span class="line">        output = torch.sigmoid(output)</span><br><span class="line">        <span class="built_in">print</span>(output.shape)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line">x = torch.randn((<span class="number">8</span>, <span class="number">207</span>, <span class="number">16</span>))</span><br><span class="line">y = torch.rand([<span class="number">8</span>,<span class="number">207</span>,<span class="number">1</span>]).<span class="built_in">float</span>()</span><br><span class="line">edge_index = torch.randint(high=<span class="number">206</span>, size=(<span class="number">2</span>, <span class="number">1200</span>))</span><br><span class="line"><span class="built_in">print</span>(edge_index.shape)</span><br><span class="line"></span><br><span class="line">edge_index = edge_index.repeat(<span class="number">8</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(edge_index.shape)</span><br><span class="line">model = Net()</span><br><span class="line">optimizer = torch.optim.Adam(params=model.parameters(),lr=<span class="number">0.01</span>)  <span class="comment"># Only perform weight-decay on first convolution.</span></span><br><span class="line"></span><br><span class="line">out = model(x,edge_index)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss = F.mse_loss(out, y)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loss.item())</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 1200])
torch.Size([8, 2, 1200])
torch.Size([8, 207, 1])
torch.Size([8, 207, 1])
torch.Size([8, 207, 1])
0.09769630432128906
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand([<span class="number">2</span>, <span class="number">2</span>, <span class="number">40000</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">    <span class="built_in">print</span>(i.shape)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([2, 40000])
torch.Size([2, 40000])
</code></pre><h1 id="批量邻接矩阵转换"><a href="#批量邻接矩阵转换" class="headerlink" title="批量邻接矩阵转换"></a>批量邻接矩阵转换</h1><h2 id="转换完stack"><a href="#转换完stack" class="headerlink" title="转换完stack"></a>转换完stack</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adj2coo</span>(<span class="params">self,Ab</span>):</span><br><span class="line">        <span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        adj = sp.coo_matrix(Ab)  <span class="comment"># 转换成coo_matrix矩阵</span></span><br><span class="line">        edge_attr = adj.data  <span class="comment"># 边权值</span></span><br><span class="line">        indices = np.vstack((adj.row, adj.col))  <span class="comment"># 我们需要的coo形式的edge_index</span></span><br><span class="line">        edge_index = torch.LongTensor(indices)  <span class="comment"># PyG需要的edge_index</span></span><br><span class="line"></span><br><span class="line">        edge_attr = torch.FloatTensor(edge_attr)  <span class="comment"># to float tensor</span></span><br><span class="line">        <span class="built_in">print</span>(edge_index.shape,edge_attr.shape)</span><br><span class="line">        <span class="keyword">return</span> edge_index,edge_attr</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edg_indexH, edge_attrH = torch.stack([self.adj2coo(i)[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> scoadj], dim=<span class="number">0</span>), torch.stack(</span><br><span class="line">            [self.adj2coo(i)[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> adj], dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="一次性解决"><a href="#一次性解决" class="headerlink" title="一次性解决"></a>一次性解决</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">adj2edge_index</span>(<span class="params">self,A</span>):</span><br><span class="line">       <span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line">       <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">       edg_indexAll = torch.zeros(<span class="number">0</span>,dtype=torch.int64)</span><br><span class="line">       edg_attrAll = torch.zeros(<span class="number">0</span>,dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> b <span class="keyword">in</span> A:</span><br><span class="line">           adj = sp.coo_matrix(b)  <span class="comment"># 转换成coo_matrix矩阵</span></span><br><span class="line">           edge_attr = adj.data  <span class="comment"># 边权值</span></span><br><span class="line">           indices = np.vstack((adj.row, adj.col))  <span class="comment"># 我们需要的coo形式的edge_index</span></span><br><span class="line">           edge_index = torch.LongTensor(indices)  <span class="comment"># PyG需要的edge_index</span></span><br><span class="line"></span><br><span class="line">           edge_attr = torch.FloatTensor(edge_attr)  <span class="comment"># to float tensor</span></span><br><span class="line">           edg_indexAll = torch.cat((edg_indexAll, edge_index))</span><br><span class="line">           edg_attrAll = torch.cat((edg_attrAll, edge_attr))</span><br><span class="line"></span><br><span class="line">       <span class="comment">#print(&quot;edg&quot;,edg_indexAll.view(A.shape[0],2,-1).shape)</span></span><br><span class="line">       <span class="comment">#print(edg_attrAll.view(A.shape[0],-1).shape)</span></span><br><span class="line">       <span class="keyword">return</span> edg_indexAll.view(A.shape[<span class="number">0</span>],<span class="number">2</span>,-<span class="number">1</span>),edg_attrAll.view(A.shape[<span class="number">0</span>],-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="模型中传参"><a href="#模型中传参" class="headerlink" title="模型中传参"></a>模型中传参</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x3 = self.relu(</span><br><span class="line">            torch.stack(</span><br><span class="line">                [self.gc6(graph, edge_index=edge_indexi, edge_attr=edge_attri) <span class="keyword">for</span> graph, edge_indexi, edge_attri <span class="keyword">in</span></span><br><span class="line">                 <span class="built_in">zip</span>(x3_h, edg_indexH, edge_attrH)],</span><br><span class="line">                dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<h3 id="在模型中分批传入adj调用adj2coo"><a href="#在模型中分批传入adj调用adj2coo" class="headerlink" title="在模型中分批传入adj调用adj2coo"></a>在模型中分批传入adj调用adj2coo</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x3 = self.relu(</span><br><span class="line">          torch.stack(</span><br><span class="line">              [self.gc6(graph, edge_index=self.adj2coo(Ab)[<span class="number">0</span>],edge_attr =self.adj2coo(Ab)[<span class="number">1</span>]) <span class="keyword">for</span> graph, Ab <span class="keyword">in</span></span><br><span class="line">               <span class="built_in">zip</span>(x3_h, scoadj)],</span><br><span class="line">              dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>图神经网络</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
</search>
