<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Learning Multimodal Violence Detection under Weak Supervision</title>
    <url>/posts/undefined/</url>
    <content><![CDATA[<h1 id="Not-only-look-but-also-listen-Learning-multimodal-violence-detection-under-weak-supervision"><a href="#Not-only-look-but-also-listen-Learning-multimodal-violence-detection-under-weak-supervision" class="headerlink" title="Not only look, but also listen: Learning multimodal violence detection under weak supervision"></a>Not only look, but also listen: Learning multimodal violence detection under weak supervision</h1><p><a href="https://arxiv.org/pdf/2007.04687.pdf">paper</a> ï¼Œ<a href="https://roc-ng.github.io/XD-Violence/">code and dataset</a></p>
<p>æš´åŠ›è§†é¢‘æ£€æµ‹æ–¹å‘è®ºæ–‡</p>
<blockquote>
<p> ğŸ’¡ We introduce a HL-Net to simultaneously capture <strong>long-range relations</strong> and<br><strong>local distance relations</strong>, of which these two relations are based on similarity prior and proximity prior, respectively</p>
<p>ä¸‰ä¸ªå¹¶è¡Œçš„branchæ•æ‰è§†é¢‘ç‰‡æ®µå’Œé›†æˆçš„ç‰¹å¾ä¹‹é—´çš„ä¸åŒè”ç³»:</p>
<ul>
<li>holistic branch captures long-range dependencies using similarity prior,</li>
<li>localized branch captures local positional relation using proximity prior,</li>
<li>score branch dynamically captures the closeness of predicted score.</li>
</ul>
</blockquote>
<span id="more"></span>
<p>ç›¸å…³å·¥ä½œï¼Œå…¶ä¸­attentionéƒ¨åˆ†å¯ä»¥åé¢çœ‹ä¸‹ï¼›</p>
<p>ä¸€äº›å·¥ä½œå°†å›¾ç¥ç»ç½‘ç»œ(GCNs)[20,39]åœ¨å›¾ä¸Šå»ºç«‹ä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»æ¨¡å‹ï¼Œå¹¶å­¦ä¹ è®¡ç®—æœºè§†è§‰çš„å¼ºå¤§è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼ŒGCNè¢«ç”¨äºæ—¶é—´æ€§åŠ¨ä½œå®šä½[50]ã€è§†é¢‘åˆ†ç±»[37,41]ã€å¼‚å¸¸æ£€æµ‹[51]ã€‚åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«[33,45]ï¼Œç‚¹äº‘è¯­ä¹‰åˆ†å‰²[21]ï¼Œå›¾åƒè¯´æ˜[46]ç­‰ç­‰ã€‚é™¤äº†GCNï¼Œæ—¶é—´å…³ç³»ç½‘ç»œ[52]ï¼Œæ—¨åœ¨å­¦ä¹ å’Œæ¨ç†è§†é¢‘å¸§ä¹‹é—´çš„æ—¶é—´ä¾èµ–å…³ç³»ï¼Œè¢«æå‡ºæ¥ç”¨äºè§£å†³è§†é¢‘åˆ†ç±»ã€‚æœ€è¿‘ï¼Œè‡ªæˆ‘æ³¨æ„ç½‘ç»œ[40,47,5,18]å·²è¢«æˆåŠŸåº”ç”¨äºè§†è§‰é—®é¢˜ã€‚æ³¨æ„åŠ›æ“ä½œå¯ä»¥é€šè¿‡èšåˆä¸€ç»„å…ƒç´ çš„ä¿¡æ¯æ¥å½±å“å•ä¸ªå…ƒç´ ï¼Œå…¶ä¸­èšåˆçš„æƒé‡æ˜¯è‡ªåŠ¨å­¦ä¹ çš„ã€‚</p>
<h1 id="å®ç°"><a href="#å®ç°" class="headerlink" title="å®ç°"></a>å®ç°</h1><h2 id="æ€»ä½“æ¡†æ¶å›¾"><a href="#æ€»ä½“æ¡†æ¶å›¾" class="headerlink" title="æ€»ä½“æ¡†æ¶å›¾"></a>æ€»ä½“æ¡†æ¶å›¾</h2><p><img src="/posts/undefined/Untitled.png" alt></p>
<ul>
<li><p>ç‰¹å¾æå–ï¼šusing the sliding window mechanism æå– è§†é¢‘ç‰¹å¾å’Œå£°éŸ³ç‰¹å¾ï¼Œåˆå¹¶æˆèåˆç‰¹å¾ï¼ˆèåˆéƒ¨åˆ†ä¸äºˆå…³æ³¨ã€‚æ»‘åŠ¨çª—å£æœºåˆ¶å¾…çœ‹ï¼‰</p>
<blockquote>
<p>Visual features: utilize two mainstream networks-C3D and I3D networks. we extract fc6 features from C3D that is pretrained on the Sports-1M dataset, and extract global_pool features from I3D pre-trained on Kinetics-200 dataset.</p>
</blockquote>
</li>
</ul>
<h2 id="Holistic-and-Localized-Networks"><a href="#Holistic-and-Localized-Networks" class="headerlink" title="Holistic and Localized Networks"></a>Holistic and Localized Networks</h2><h3 id="Holistic-ç‰¹å¾ç›¸ä¼¼æ€§"><a href="#Holistic-ç‰¹å¾ç›¸ä¼¼æ€§" class="headerlink" title="Holistic - ç‰¹å¾ç›¸ä¼¼æ€§"></a><strong>Holistic - ç‰¹å¾ç›¸ä¼¼æ€§</strong></h3><hr>
<p>é€šç”¨çš„å›¾å·ç§¯è¡¨ç¤ºå¯ä»¥çœ‹ä¸ºï¼š</p>
<script type="math/tex; mode=display">
X_l+1 = Update(Aggregate(X_l,W_l^{agg},W_l^{update}))</script><ul>
<li><strong>GCNèŒƒå¼ï¼š</strong><script type="math/tex; mode=display">
X^H_{l+1} = Dropout(ReLU(A^HX^H_lW^H_l))</script></li>
</ul>
<p>æ³¨ï¼š$X^H_0 = X^S_0 = X^L_0 = X^F$</p>
<p><strong>ç‰¹å¾ç›¸ä¼¼æ€§çš„é‚»æ¥çŸ©é˜µè¡¨ç¤º:</strong></p>
<script type="math/tex; mode=display">
A^H_{ij} = g(f(x_i,x_j))</script><p>$A^H_{ij}$è¡¡é‡ç¬¬iä¸ªå’Œç¬¬jä¸ªç‰¹å¾çš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼›gæ˜¯å½’ä¸€åŒ–å‡½æ•°ï¼Œfå‡½æ•°è®¡ç®—ä¸€å¯¹ç‰¹å¾çš„ç›¸ä¼¼æ€§ï¼›é™„å½•éƒ¨åˆ†è¿˜è®¨è®ºäº†ä¸€ä¸‹å…¶ä»–ç‰ˆæœ¬çš„ã€‚æœ¬æ–‡å®šä¹‰å¦‚ä¸‹</p>
<ul>
<li>f å®šä¹‰ä¸ºï¼š</li>
</ul>
<script type="math/tex; mode=display">
f(x_i,x_j) = \frac{x_i^Tx_j}{||x_i||_{2} \\ . \\ ||x_j||_2}</script><ul>
<li>thresholdingæ“ä½œï¼ˆ$\tau$æ˜¯threshodï¼Œfå°†ç›¸ä¼¼æ€§é™åˆ¶åœ¨ï¼ˆ0,1]ä¹‹é—´ï¼‰ï¼š</li>
</ul>
<script type="math/tex; mode=display">
f(x_i,x_j) = \begin{cases} f(x_i,x_j)  & f(x_i,x_j) > \tau \\ 0 & f(x_i,x_j)\leq \tau\end{cases}</script><ul>
<li><p>ä½¿ç”¨softmaxä½œä¸ºå½’ä¸€åŒ–å‡½æ•°gï¼Œä½¿å¾—Açš„æ¯ä¸€è¡Œçš„å’Œéƒ½ä¸º1ã€‚</p>
<script type="math/tex; mode=display">
A^H_{ij} = \frac{exp(A^H_{ij})}{\sum ^{T'}_{k=1} exp(A^H_{ij})}</script></li>
</ul>
<h3 id="localized-branch-proximity-prior-å’Œæ—¶é—´ä¸€è‡´æ€§ç±»ä¼¼ï¼‰"><a href="#localized-branch-proximity-prior-å’Œæ—¶é—´ä¸€è‡´æ€§ç±»ä¼¼ï¼‰" class="headerlink" title="localized branch -proximity prior (å’Œæ—¶é—´ä¸€è‡´æ€§ç±»ä¼¼ï¼‰"></a><strong>localized branch -proximity prior (å’Œæ—¶é—´ä¸€è‡´æ€§ç±»ä¼¼ï¼‰</strong></h3><hr>
<script type="math/tex; mode=display">
A^L_{ij} = exp(\frac{-|i-j|^r}{\sigma })</script><h2 id="Online-detection-amp-score-branch"><a href="#Online-detection-amp-score-branch" class="headerlink" title="Online detection &amp; score branch"></a>Online detection &amp; score branch</h2><p>æ­£å¦‚æˆ‘ä»¬æåˆ°çš„ï¼Œæš´åŠ›æ£€æµ‹ç³»ç»Ÿä¸ä»…é€‚ç”¨äºç¦»çº¿æ£€æµ‹ï¼ˆäº’è”ç½‘å½•åƒæœºï¼‰ï¼Œä¹Ÿé€‚ç”¨äºåœ¨çº¿æ£€æµ‹ï¼ˆç›‘æ§ç³»ç»Ÿï¼‰ã€‚ç„¶è€Œï¼Œ<strong>ä¸Šè¿°HL-Net</strong>çš„åœ¨çº¿æ£€æµ‹å—åˆ°äº†ä¸€ä¸ªä¸»è¦éšœç¢çš„é˜»ç¢ï¼š<strong>HL-Netéœ€è¦æ•´ä¸ªè§†é¢‘æ¥è·å¾—é•¿è·ç¦»çš„ä¾èµ–å…³ç³»</strong>ã€‚ä¸ºäº†è·³å‡ºè¿™ä¸ªå›°å¢ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªHLCè¿‘ä¼¼å™¨ï¼ŒåªæŠŠä»¥å‰çš„è§†é¢‘ç‰‡æ®µä½œä¸ºè¾“å…¥ï¼Œåœ¨HL-Netçš„æŒ‡å¯¼ä¸‹äº§ç”Ÿç²¾ç¡®çš„é¢„æµ‹ã€‚ä¸¤ä¸ªå †å çš„FCå±‚å’ŒReLUä»¥åŠä¸€ä¸ªä¸€ç»´å› æœå·ç§¯å±‚æ„æˆäº†HLCè¿‘ä¼¼å™¨ã€‚ä¸€ç»´å› æœå·ç§¯å±‚çš„æ ¸å¤§å°ä¸º5ï¼Œè·¨åº¦ä¸º1ï¼Œåœ¨æ—¶é—´ä¸Šæ»‘åŠ¨å·ç§¯æ»¤æ³¢å™¨ã€‚ä¸€ç»´å› æœå·ç§¯å±‚ä¹Ÿå……å½“åˆ†ç±»å™¨ï¼Œå…¶è¾“å‡ºæ˜¯å½¢çŠ¶ä¸º Tâ€™ çš„æš´åŠ›æ¿€æ´»è¡¨ç¤ºä¸º$C^S$ã€‚æ›´å¦™çš„æ˜¯ï¼Œè¿™ä¸ªæ“ä½œå¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„åˆ†æ”¯ï¼Œåä¸º<strong>åŠ¨æ€å¾—åˆ†åˆ†æ”¯(dynamic score branch)</strong>ï¼Œä»¥æ‰©å±•HL-Netï¼Œå®ƒå–å†³äº$C^S$ã€‚</p>
<h3 id="score-branch"><a href="#score-branch" class="headerlink" title="score branch"></a>score branch</h3><p>ç”¨äºonline detection, è§£å†³éœ€è¦å°†æ•´ä¸ªè§†é¢‘ä½œä¸ºè¾“å…¥ï¼ˆä»¥è·å¾—é•¿è·ç¦»ä¾èµ–ï¼‰çš„é—®é¢˜ã€‚</p>
<p>è¯¥åˆ†æ”¯çš„ä¸»è¦ä½œç”¨æ˜¯å°†ä¸€ä¸ªä½ç½®çš„å“åº”è®¡ç®—ä¸ºæ‰€æœ‰ä½ç½®ç‰¹å¾çš„åŠ æƒå’Œï¼Œå…¶ä¸­æƒé‡å–å†³äºåˆ†æ•°çš„æ¥è¿‘ç¨‹åº¦ã€‚ä¸æ•´ä½“å’Œå±€éƒ¨åˆ†æ”¯çš„å…³ç³»çŸ©é˜µä¸åŒï¼Œåˆ†æ•°åˆ†æ”¯çš„å…³ç³»çŸ©é˜µåœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šæ›´æ–°ï¼Œå¹¶ä¸”å–å†³äºé¢„æµ‹çš„åˆ†æ•°è€Œä¸æ˜¯å…ˆéªŒã€‚ä»å½¢å¼ä¸Šçœ‹ï¼Œåˆ†æ•°åˆ†æ”¯çš„å…³ç³»çŸ©é˜µè®¾è®¡å¦‚ä¸‹ï¼š</p>
<script type="math/tex; mode=display">
A^S_{ij} = \rho(1 - |s(C^S_i) - s(C^S_j)|)</script><p>sæ˜¯ sigmoid, å‡½æ•°Ïç”¨äºåŠ å¼ºï¼ˆå’Œå‰Šå¼±ï¼‰å¾—åˆ†æ¥è¿‘åº¦å¤§äºï¼ˆå’Œå°äºï¼‰0.5çš„é…å¯¹å…³ç³»ï¼Œsoftmaxä¹Ÿç”¨äºå½’ä¸€åŒ–ã€‚</p>
<script type="math/tex; mode=display">
\rho (x) = \frac{1}{1+exp(-\frac{x-0.5}{0.1})}</script><h2 id="training-based-on-MIL"><a href="#training-based-on-MIL" class="headerlink" title="training based on MIL"></a>training based on MIL</h2><script type="math/tex; mode=display">
C^P = (X^H||X^L||X^S)W</script><p>æ‰€æœ‰è¾“å‡º($C^P å’Œ C^S$)åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„K-maxå–å¹³å‡ä½œä¸ºè¾“å‡ºï¼Œä»¥å¾—åˆ°$y^P$ å’Œ$y^S$ã€‚Kå®šä¹‰ä¸ºTâ€˜ é™¤ä»¥qåŠ ä¸€åå‘ä¸‹å–æ•´ã€‚</p>
<p>è¡¥å……ï¼š top-kç­–ç•¥ï¼ˆWeakly-supervised video anomaly detection with robust temporal feature magnitude learningï¼‰</p>
<ul>
<li>lossç»“åˆä¸‰ä¸ªbranch</li>
</ul>
<blockquote>
<p>The instances corresponding to the K-max activation in the positive bag is most likely to be true positive instances (violence). The instances corresponding to the K-max activation in the negative bag is hard instances. We expect these two types of instances to be as far as possible.</p>
</blockquote>
<p>L_BCEï¼ˆbinary crossentropy) å’ŒL_BCE2åˆ†åˆ«å¯¹åº”ä¸º$y^p å’Œ y^s$ ä¸ ground truth yä¹‹é—´çš„lossã€‚L_DISTILLä¸ºçŸ¥è¯†è’¸é¦æŸå¤±ã€‚</p>
<script type="math/tex; mode=display">
L_{TOTAL}  = L_{BCE} + L_{BCE2} +\lambda L_{DISTILL}</script><script type="math/tex; mode=display">
L_{DISTILL} = \sum \limits^N_{j=1} (- \sum \limits_i s(C^P_i) log (s(C_i^S)))</script><h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>æ–¹æ³•æ”¯æŒçº¿ä¸Šå’Œç¦»çº¿çš„æ£€æµ‹ã€‚sigmoidå‡½æ•°ä½œä¸º$C^P$å’Œ$C^S$çš„æ¿€æ´»å‡½æ•°ï¼Œå¹¶æœ€åç”Ÿæˆåœ¨[0,1]ä¹‹é—´çš„æš´åŠ›ç½®ä¿¡å¾—åˆ†ã€‚æ³¨ï¼šåœ¨çº¿ä¸Šé¢„æµ‹ä¸­ï¼Œåªæœ‰HLCè¿‘ä¼¼å™¨å·¥ä½œï¼ŒHL-NETå¯ä»¥ç§»é™¤ã€‚</p>
<h1 id="å®éªŒ"><a href="#å®éªŒ" class="headerlink" title="å®éªŒ"></a>å®éªŒ</h1><h2 id="è¯„ä¼°æ ‡å‡†"><a href="#è¯„ä¼°æ ‡å‡†" class="headerlink" title="è¯„ä¼°æ ‡å‡†"></a>è¯„ä¼°æ ‡å‡†</h2><p>we utilize the frame-level <strong>precision-recall curve</strong> (PRC) and corresponding area under the curve <strong>(average precision, AP)</strong> [30] rather than receiver operating characteristic curve (ROC) and corresponding area under the curve (AUC) [44,43]<br><strong>since AUC usually shows an optimistic result when dealing with class-imbalanced data, and PRC and AP focus on positive samples (violence)</strong></p>
<ul>
<li>Precision and Recall (PRæ›²çº¿)ï¼šç”¨äºç¨€æœ‰äº‹ä»¶æ£€æµ‹ï¼Œå¦‚ç›®æ ‡æ£€æµ‹ã€ä¿¡æ¯æ£€ç´¢ã€æ¨èç³»ç»Ÿã€‚è´Ÿæ ·æœ¬å¾ˆå¤šçš„æ—¶å€™ï¼Œ??? = FPâ„(FP+TNï¼‰å¾ˆå°ï¼Œæ¯”è¾ƒTPRå’ŒFPRæ²¡æœ‰å¤ªå¤§æ„ä¹‰ï¼ˆROCï¼‰</li>
</ul>
<h2 id="æ€§èƒ½è¡¨ç°"><a href="#æ€§èƒ½è¡¨ç°" class="headerlink" title="æ€§èƒ½è¡¨ç°"></a>æ€§èƒ½è¡¨ç°</h2><p><img src="/posts/undefined/Untitled 1.png" alt></p>
<p>ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ˆ20å¹´çš„æ–‡ç« ï¼‰</p>
<p>æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æˆ‘ä»¬çš„æš´åŠ›æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒC3Dæ¯”I3Då·®äº†å¾ˆå¤§ä¸€æˆªã€‚</p>
<h2 id="æ¶ˆèå®éªŒ"><a href="#æ¶ˆèå®éªŒ" class="headerlink" title="æ¶ˆèå®éªŒ"></a>æ¶ˆèå®éªŒ</h2><ul>
<li>äº”ç§æ¨¡æ€APå¯¹æ¯”</li>
</ul>
<p><img src="/posts/undefined/Untitled 2.png" alt="Untitled"></p>
<p>ï¼ˆè¯¥æ–‡è¯æ˜å£°éŸ³å’Œè§†è§‰çš„èåˆæ£€æµ‹æ•ˆæœæ›´å¥½ï¼Œä¸”è§†è§‰æ¨¡æ€çš„ä½œç”¨ä¼˜äºå£°éŸ³ï¼‰</p>
<ul>
<li><p>ä¸‰ä¸ªåˆ†æ”¯çš„å¯¹æ¯”ï¼ˆholistic, localized and score branchesï¼‰</p>
<ul>
<li>ä¸‰ä¸ªåˆ†æ”¯å•ç‹¬çš„æƒ…å†µè¡¨ç°ç›¸ä¼¼</li>
<li>ç§»é™¤ä»»ä½•ä¸€ä¸ªåˆ†æ”¯éƒ½ä¼šä½¿å¾—è¡¨ç°å˜å·®</li>
<li>HL-NETåœ¨è¿™ä¸‰ä¸ªåˆ†æ”¯ä¸€èµ·ä½œç”¨çš„æ—¶å€™è¡¨ç°æœ€å¥½ï¼Œå› æ­¤è¯æ˜ä¸‰ä¸ªåˆ†æ”¯éƒ½ä¸å¯æ›¿ä»£ã€‚</li>
</ul>
<p><img src="/posts/undefined/Untitled 3.png" alt></p>
</li>
<li><p>online vs offline</p>
<p><img src="/posts/undefined/Untitled 4.png" alt></p>
</li>
</ul>
<h1 id="å…¶ä»–"><a href="#å…¶ä»–" class="headerlink" title="å…¶ä»–"></a>å…¶ä»–</h1><h3 id="å…‰æµ"><a href="#å…‰æµ" class="headerlink" title="å…‰æµ"></a>å…‰æµ</h3><blockquote>
<p>å…‰æµï¼ˆoptical flowï¼‰æ˜¯ç©ºé—´è¿åŠ¨ç‰©ä½“åœ¨è§‚å¯Ÿæˆåƒå¹³é¢ä¸Šçš„åƒç´ è¿åŠ¨çš„ç¬æ—¶é€Ÿåº¦ã€‚<br>å…‰æµæ³•æ˜¯åˆ©ç”¨å›¾åƒåºåˆ—ä¸­åƒç´ åœ¨æ—¶é—´åŸŸä¸Šçš„å˜åŒ–ä»¥åŠç›¸é‚»å¸§ä¹‹é—´çš„ç›¸å…³æ€§æ¥æ‰¾åˆ°ä¸Šä¸€å¸§è·Ÿå½“å‰å¸§ä¹‹é—´å­˜åœ¨çš„å¯¹åº”å…³ç³»ï¼Œä»è€Œè®¡ç®—å‡ºç›¸é‚»å¸§ä¹‹é—´ç‰©ä½“çš„è¿åŠ¨ä¿¡æ¯çš„ä¸€ç§æ–¹æ³•ã€‚<br>é€šå¸¸å°†äºŒç»´å›¾åƒå¹³é¢ç‰¹å®šåæ ‡ç‚¹ä¸Šçš„ç°åº¦ç¬æ—¶å˜åŒ–ç‡å®šä¹‰ä¸ºå…‰æµçŸ¢é‡ã€‚</p>
</blockquote>
<p>ä½œè€…åœ¨ã€ŠOn the Integration of Optical Flow and Action Recognitionã€‹è¿™ç¯‡æ–‡ç« [1]ä¸­æ·±å…¥è®¨è®ºäº†å…‰æµä¸è¡Œä¸ºè¯†åˆ«çš„ç»“åˆï¼Œå¹¶é€šè¿‡å®éªŒè§‚å¯Ÿåˆ°å¦‚ä¸‹ç»“è®ºï¼š<br>ï¼ˆ1ï¼‰å…‰æµå¯¹äºè¡Œä¸ºè¯†åˆ«æ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºå®ƒçš„å¤–è§‚ä¸å˜æ€§ï¼›<br>ï¼ˆ2ï¼‰å…‰æµæ³•é‡‡ç”¨æœ€å°åŒ–ç«¯ç‚¹è¯¯å·®ï¼ˆEPEï¼Œend-point-errorï¼‰æ¥ä¼˜åŒ–ï¼Œä½†æ˜¯å½“å‰EPEæ–¹æ³•ä¸åŠ¨ä½œè¯†åˆ«æ€§èƒ½æ²¡æœ‰å¾ˆå¥½çš„ç›¸å…³æ€§ï¼›<br>ï¼ˆ3ï¼‰å¯¹äºæµ‹è¯•è¿‡çš„å…‰æµæ–¹æ³•ï¼Œåœ¨è¾¹ç•Œä¸Šå’Œå°ä½ç§»ä¸Šçš„ç²¾åº¦ä¸åŠ¨ä½œè¯†åˆ«æ€§èƒ½æœ€ç›¸å…³ï¼›<br>ï¼ˆ4ï¼‰é‡‡ç”¨æœ€å°åŒ–åˆ†ç±»è¯¯å·®ï¼ˆè€ŒéEPEï¼‰æ¥è®­ç»ƒå…‰æµå¯ä»¥æé«˜è¯†åˆ«æ€§èƒ½ï¼›<br>ï¼ˆ5ï¼‰ç”¨äºè¡Œä¸ºè¯†åˆ«ä»»åŠ¡çš„å…‰æµä¸åŒäºä¼ ç»Ÿçš„å…‰æµï¼Œç‰¹åˆ«æ˜¯åœ¨äººä½“å†…éƒ¨å’Œèº«ä½“è¾¹ç•Œå¤„ã€‚<br>åŸæ–‡é“¾æ¥ï¼š<a href="https://blog.csdn.net/zhang_can/article/details/80259946">https://blog.csdn.net/zhang_can/article/details/80259946</a></p>
]]></content>
      <categories>
        <category>è§†é¢‘æ£€æµ‹</category>
      </categories>
      <tags>
        <tag>æ·±åº¦è§†é¢‘æ£€æµ‹</tag>
        <tag>æš´åŠ›è§†é¢‘æ£€æµ‹</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo Hello World</title>
    <url>/posts/1e44dbaf/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>hexo æ–‡æ¡£</category>
      </categories>
  </entry>
  <entry>
    <title>pytorchlearning</title>
    <url>/posts/643f7c73/</url>
    <content><![CDATA[<h1 id="pytorch-å­¦ä¹ è®°å½•"><a href="#pytorch-å­¦ä¹ è®°å½•" class="headerlink" title="pytorch å­¦ä¹ è®°å½•"></a>pytorch å­¦ä¹ è®°å½•</h1><p>å¾…æ›´â€¦</p>
<p>è§†é¢‘å­¦ä¹ ç¬”è®°githubå­˜å‚¨ä»“åº“ï¼šï¼ˆ@ljhï¼‰</p>
<p><a href="https://github.com/Xandra298/Pytorchlearning">Xandra298/Pytorchlearning (github.com)</a></p>
]]></content>
      <categories>
        <category>æ·±åº¦å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>ç¬¬ä¸€æ¬¡æ­å»ºhexoåšå®¢</title>
    <url>/posts/83682157/</url>
    <content><![CDATA[<h2 id="hexo-deploy-åˆ°githubå¤±è´¥"><a href="#hexo-deploy-åˆ°githubå¤±è´¥" class="headerlink" title="hexo deploy åˆ°githubå¤±è´¥"></a>hexo deploy åˆ°githubå¤±è´¥</h2><p>ä½¿ç”¨deployä¹‹åè¦æ±‚è¾“å…¥è´¦æˆ·å’Œå¯†ç ï¼Œå¯†ç æ­£ç¡®ä½†æ˜¯æ˜¾ç¤ºéªŒè¯å¤±è´¥ã€‚</p>
<span id="more"></span>
<ul>
<li><p>å¤±è´¥åŸå› ï¼š</p>
<p>github 21å¹´å–æ¶ˆäº†å¯†ç éªŒè¯</p>
<p><img src="/posts/83682157/image-20221117162546351.png" alt="image-20221117162546351"></p>
</li>
<li><p>è§£å†³æ–¹å¼</p>
<p>åœ¨_config.ymlçš„deployä¸­ï¼Œrepoå¯¹åº”çš„å¡«å†™ä¸ºgithubä»“åº“çš„sshåœ°å€ï¼Œå¯ä»¥é¿å…è¿›è¡Œå¯†ç éªŒè¯ï¼ˆå‰æï¼šgité…ç½®å®Œæˆï¼Œgithubä¸Šæœ‰è®¾ç½®å…¬é’¥ï¼‰</p>
</li>
</ul>
<h2 id="å®Œæˆçš„é…ç½®"><a href="#å®Œæˆçš„é…ç½®" class="headerlink" title="å®Œæˆçš„é…ç½®"></a>å®Œæˆçš„é…ç½®</h2><ul>
<li><p>ä½¿ç”¨hexoä¸»é¢˜</p>
<p><a href="https://github.com/probberechts/hexo-theme-cactus">hexo-theme-cactus</a></p>
<ul>
<li><a href="https://github.com/theme-next/hexo-theme-next">theme-next/hexo-theme-next: Elegant and powerful theme for Hexo. (github.com)</a></li>
</ul>
</li>
<li><p>ä¸»é¢˜é…ç½® tags å’Œ categories</p>
</li>
<li><p>é…ç½®baidu_analytics</p>
</li>
<li><p>comment</p>
<ul>
<li>ä½¿ç”¨utterancesè®¾ç½®comment</li>
<li>æ¥å¿…åŠ›</li>
</ul>
</li>
<li><p>ä¸è’œå­ç»Ÿè®¡</p>
</li>
<li><p>ä½¿ç”¨ä¸»é¢˜çš„search</p>
<ul>
<li>ä¸æˆåŠŸ-&gt;è§£å†³ï¼šå‘ç°æ˜¯npm installçš„æ—¶å€™æ²¡æœ‰åœ¨åšå®¢æ ¹ç›®å½•å¯¼è‡´çš„é—®é¢˜</li>
</ul>
</li>
<li><p>è§£å†³å›¾ç‰‡è·¯å¾„é—®é¢˜</p>
<ul>
<li><p>sources/imagesç›®å½•ä¸‹å­˜å¼•å›¾ç‰‡</p>
</li>
<li><p>setupé‚£ç¯‡postä½¿ç”¨post_asset_folder: trueçš„æ–¹å¼ï¼ˆéœ€è¦å®‰è£…hexoæ’ä»¶ï¼‰ï¼Œå°†èµ„æºæ”¾åœ¨å¯¹åº”ç›®å½•ä¸‹</p>
<ul>
<li><p>æœ¬åœ°æˆåŠŸä½†æ˜¯æœåŠ¡å™¨æ˜¾ç¤ºæ— æ•ˆï¼Œé‡æ–°å®‰è£…æ’ä»¶æˆåŠŸï¼š</p>
<p><code>npm install https://github.com/7ym0n/hexo-asset-image --save</code></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>hexo new postå¤±è´¥</p>
<ul>
<li>ä½¿ç”¨ hexo new â€˜[post]â€™ â€œpostnameâ€</li>
</ul>
</li>
<li><p>æ•°å­¦å…¬å¼æ¸²æŸ“</p>
<ul>
<li><a href="https://blog.csdn.net/qq_38496329/article/details/104065659">https://blog.csdn.net/qq_38496329/article/details/104065659</a></li>
<li>æ³¨æ„åœ¨æ–‡ç« ä¸­front éƒ¨åˆ†è®¾ç½®mathjax: true</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>éšç¬”</category>
      </categories>
      <tags>
        <tag>è®°å½•</tag>
      </tags>
  </entry>
  <entry>
    <title>é‚»æ¥çŸ©é˜µè¡¨ç¤ºæˆPyGéœ€è¦çš„edge_indexå¹¶è¿›è¡Œå¸¦è¾¹æƒçš„ç½‘ç»œæ„å»º</title>
    <url>/posts/35a1541/</url>
    <content><![CDATA[<h1 id="é‚»æ¥çŸ©é˜µ-to-pygéœ€è¦çš„edge-indexæ ¼å¼"><a href="#é‚»æ¥çŸ©é˜µ-to-pygéœ€è¦çš„edge-indexæ ¼å¼" class="headerlink" title="é‚»æ¥çŸ©é˜µ to pygéœ€è¦çš„edge_indexæ ¼å¼"></a>é‚»æ¥çŸ©é˜µ to pygéœ€è¦çš„edge_indexæ ¼å¼</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">A = torch.rand([<span class="number">10</span>,<span class="number">10</span>]) <span class="comment"># 10*10çš„é‚»æ¥çŸ©é˜µAï¼Œå¸¦æœ‰æƒå€¼ï¼Œè€Œé0/1</span></span><br><span class="line">A</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<pre><code>tensor([[0.1155, 0.9638, 0.2571, 0.9226, 0.8682, 0.7622, 0.2149, 0.2946, 0.7223,
         0.3560],
        [0.9516, 0.4642, 0.4714, 0.9184, 0.4347, 0.2043, 0.5894, 0.4400, 0.0703,
         0.2901],
        [0.0824, 0.4602, 0.8865, 0.3120, 0.4492, 0.0532, 0.8142, 0.0837, 0.0038,
         0.3747],
        [0.5422, 0.6572, 0.5248, 0.5290, 0.1992, 0.5341, 0.5759, 0.6696, 0.7628,
         0.4712],
        [0.3346, 0.3686, 0.0381, 0.3873, 0.6583, 0.1527, 0.3304, 0.6655, 0.8988,
         0.8470],
        [0.6421, 0.4518, 0.3996, 0.8583, 0.3487, 0.2303, 0.4669, 0.5157, 0.3406,
         0.8329],
        [0.0353, 0.7500, 0.4662, 0.1905, 0.9047, 0.5608, 0.0301, 0.2665, 0.3959,
         0.8637],
        [0.4151, 0.6476, 0.3839, 0.5290, 0.8004, 0.0158, 0.1677, 0.2532, 0.2029,
         0.8267],
        [0.3447, 0.2534, 0.4317, 0.1251, 0.5761, 0.8150, 0.3714, 0.9375, 0.0819,
         0.6156],
        [0.8898, 0.9042, 0.9869, 0.5295, 0.0037, 0.8648, 0.2097, 0.4836, 0.9336,
         0.9813]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line">adj  = sp.coo_matrix(A) <span class="comment">#è½¬æ¢æˆcoo_matrixçŸ©é˜µ</span></span><br><span class="line">values = adj.data</span><br><span class="line">values,adj.row,adj.col</span><br></pre></td></tr></table></figure>
<pre><code>(array([0.115466  , 0.9637896 , 0.25710517, 0.92260057, 0.8682389 ,
        0.76218355, 0.21490818, 0.29461628, 0.7223401 , 0.35596448,
        0.95159733, 0.46416718, 0.47142482, 0.91838354, 0.43471956,
        0.20431256, 0.5893517 , 0.4400227 , 0.07025671, 0.29014403,
        0.08241057, 0.46023846, 0.8864814 , 0.3120287 , 0.44915515,
        0.05320156, 0.8142431 , 0.0837118 , 0.00377417, 0.37474608,
        0.5421733 , 0.6572476 , 0.5247708 , 0.52899545, 0.19917059,
        0.53414476, 0.57589775, 0.66963714, 0.7627656 , 0.4711967 ,
        0.33464396, 0.36859566, 0.03808755, 0.38733852, 0.65829617,
        0.15265721, 0.33041573, 0.6654919 , 0.8987522 , 0.84703857,
        0.64209276, 0.45184284, 0.39958882, 0.8583167 , 0.3487457 ,
        0.2302674 , 0.46692657, 0.51570326, 0.34056902, 0.8329418 ,
        0.0352906 , 0.74997294, 0.4662227 , 0.19050717, 0.90473145,
        0.560828  , 0.03013605, 0.2665158 , 0.39589775, 0.86372095,
        0.41512853, 0.64762414, 0.38388795, 0.5290383 , 0.8003788 ,
        0.01581752, 0.16768086, 0.2532009 , 0.20286953, 0.82665706,
        0.34473598, 0.25343996, 0.43165004, 0.12514311, 0.5760847 ,
        0.8149595 , 0.37137532, 0.93750864, 0.08186364, 0.6156229 ,
        0.8898397 , 0.9041761 , 0.986942  , 0.529477  , 0.00366998,
        0.86481035, 0.20966154, 0.4836309 , 0.93361455, 0.9812884 ],
       dtype=float32),
 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,
        4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,
        6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,
        8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], dtype=int32),
 array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
        2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,
        4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,
        6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,
        8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">indices = np.vstack((adj.row,adj.col)) <span class="comment"># æˆ‘ä»¬éœ€è¦çš„cooå½¢å¼çš„edge_index</span></span><br><span class="line">edge_index = torch.LongTensor(indices)<span class="comment">#PyGéœ€è¦çš„edge_index</span></span><br><span class="line">edge_index,edge_index.shape</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,
          2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,
          4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,
          7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,
          9, 9, 9, 9],
         [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,
          4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,
          8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,
          2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,
          6, 7, 8, 9]]),
 torch.Size([2, 100]))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">edge_attr = adj.data <span class="comment">#è¾¹æƒå€¼</span></span><br><span class="line">edge_attr = torch.FloatTensor(edge_attr)<span class="comment">#to float tensor</span></span><br><span class="line">edge_attr,edge_attr.shape</span><br></pre></td></tr></table></figure>
<pre><code>(tensor([4.5977e-01, 6.6455e-01, 4.3946e-01, 3.8642e-01, 1.2331e-01, 2.9945e-01,
         2.5433e-01, 9.7476e-01, 4.5961e-04, 5.9594e-02, 2.2455e-01, 9.7698e-01,
         8.7531e-01, 2.8142e-01, 7.0980e-01, 6.2595e-01, 2.3625e-01, 5.7737e-01,
         4.4227e-01, 6.5420e-01, 5.4512e-01, 2.4614e-01, 6.9270e-01, 6.8005e-01,
         1.3384e-01, 5.9974e-01, 9.2275e-01, 3.6578e-01, 3.5667e-01, 5.8081e-01,
         9.6142e-02, 8.5471e-01, 5.9899e-02, 3.0163e-01, 2.9641e-01, 2.8706e-01,
         4.8757e-01, 8.8466e-01, 3.4357e-01, 9.9034e-01, 4.5909e-01, 7.2475e-01,
         2.4294e-01, 7.3560e-01, 3.2247e-01, 7.6749e-01, 3.6008e-01, 3.0816e-01,
         7.4665e-01, 6.7713e-01, 6.6836e-01, 8.9111e-01, 8.0428e-01, 7.9984e-01,
         6.5296e-01, 8.1743e-01, 8.8702e-01, 3.6678e-01, 4.2774e-01, 2.3170e-02,
         8.1350e-01, 1.6834e-01, 7.7933e-02, 3.8021e-01, 9.7750e-01, 5.6143e-01,
         7.9341e-01, 3.7514e-01, 9.3114e-01, 5.6821e-01, 8.4002e-01, 9.2273e-01,
         5.6649e-01, 7.5386e-01, 9.1587e-01, 3.9596e-02, 8.9435e-01, 5.6476e-01,
         2.3289e-01, 1.9653e-01, 2.1682e-01, 2.8950e-01, 7.5310e-01, 6.7648e-01,
         5.1057e-02, 1.6519e-01, 5.8807e-01, 9.4542e-02, 6.3111e-01, 2.9049e-01,
         5.7742e-02, 3.1503e-01, 5.6936e-01, 2.2748e-01, 4.8668e-01, 6.4949e-01,
         6.1752e-01, 3.9269e-01, 2.7897e-01, 5.5806e-01]),
 torch.Size([100]))
</code></pre><h1 id="edge-index-to-é‚»æ¥çŸ©é˜µ"><a href="#edge-index-to-é‚»æ¥çŸ©é˜µ" class="headerlink" title="edge_index to é‚»æ¥çŸ©é˜µ"></a>edge_index to é‚»æ¥çŸ©é˜µ</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> sparse_coo_tensor</span><br><span class="line">adj = sparse_coo_tensor(edge_index,edge_attr,[<span class="number">10</span>,<span class="number">10</span>])</span><br><span class="line">adj.to_dense()</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[4.5977e-01, 6.6455e-01, 4.3946e-01, 3.8642e-01, 1.2331e-01, 2.9945e-01,
         2.5433e-01, 9.7476e-01, 4.5961e-04, 5.9594e-02],
        [2.2455e-01, 9.7698e-01, 8.7531e-01, 2.8142e-01, 7.0980e-01, 6.2595e-01,
         2.3625e-01, 5.7737e-01, 4.4227e-01, 6.5420e-01],
        [5.4512e-01, 2.4614e-01, 6.9270e-01, 6.8005e-01, 1.3384e-01, 5.9974e-01,
         9.2275e-01, 3.6578e-01, 3.5667e-01, 5.8081e-01],
        [9.6142e-02, 8.5471e-01, 5.9899e-02, 3.0163e-01, 2.9641e-01, 2.8706e-01,
         4.8757e-01, 8.8466e-01, 3.4357e-01, 9.9034e-01],
        [4.5909e-01, 7.2475e-01, 2.4294e-01, 7.3560e-01, 3.2247e-01, 7.6749e-01,
         3.6008e-01, 3.0816e-01, 7.4665e-01, 6.7713e-01],
        [6.6836e-01, 8.9111e-01, 8.0428e-01, 7.9984e-01, 6.5296e-01, 8.1743e-01,
         8.8702e-01, 3.6678e-01, 4.2774e-01, 2.3170e-02],
        [8.1350e-01, 1.6834e-01, 7.7933e-02, 3.8021e-01, 9.7750e-01, 5.6143e-01,
         7.9341e-01, 3.7514e-01, 9.3114e-01, 5.6821e-01],
        [8.4002e-01, 9.2273e-01, 5.6649e-01, 7.5386e-01, 9.1587e-01, 3.9596e-02,
         8.9435e-01, 5.6476e-01, 2.3289e-01, 1.9653e-01],
        [2.1682e-01, 2.8950e-01, 7.5310e-01, 6.7648e-01, 5.1057e-02, 1.6519e-01,
         5.8807e-01, 9.4542e-02, 6.3111e-01, 2.9049e-01],
        [5.7742e-02, 3.1503e-01, 5.6936e-01, 2.2748e-01, 4.8668e-01, 6.4949e-01,
         6.1752e-01, 3.9269e-01, 2.7897e-01, 5.5806e-01]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adj = sp.coo_matrix((edge_attr,(edge_index[<span class="number">0</span>],edge_index[<span class="number">1</span>])),shape=[<span class="number">10</span>,<span class="number">10</span>])</span><br><span class="line">adj.toarray()</span><br></pre></td></tr></table></figure>
<pre><code>array([[4.5977378e-01, 6.6455245e-01, 4.3945801e-01, 3.8642406e-01,
        1.2331247e-01, 2.9944807e-01, 2.5433010e-01, 9.7475851e-01,
        4.5961142e-04, 5.9593856e-02],
       [2.2454953e-01, 9.7697508e-01, 8.7531334e-01, 2.8141612e-01,
        7.0980257e-01, 6.2595367e-01, 2.3624879e-01, 5.7737088e-01,
        4.4226754e-01, 6.5420014e-01],
       [5.4512197e-01, 2.4613553e-01, 6.9269532e-01, 6.8004644e-01,
        1.3383734e-01, 5.9973723e-01, 9.2274553e-01, 3.6578351e-01,
        3.5666680e-01, 5.8080733e-01],
       [9.6142113e-02, 8.5471165e-01, 5.9899449e-02, 3.0162632e-01,
        2.9641372e-01, 2.8705674e-01, 4.8757398e-01, 8.8466209e-01,
        3.4356719e-01, 9.9034435e-01],
       [4.5909441e-01, 7.2474545e-01, 2.4293584e-01, 7.3560286e-01,
        3.2246715e-01, 7.6749289e-01, 3.6007798e-01, 3.0815858e-01,
        7.4665487e-01, 6.7713338e-01],
       [6.6836429e-01, 8.9111018e-01, 8.0427557e-01, 7.9984426e-01,
        6.5295666e-01, 8.1743485e-01, 8.8702154e-01, 3.6678237e-01,
        4.2774427e-01, 2.3170471e-02],
       [8.1350172e-01, 1.6834372e-01, 7.7932715e-02, 3.8021082e-01,
        9.7749555e-01, 5.6143039e-01, 7.9341477e-01, 3.7514049e-01,
        9.3114382e-01, 5.6820768e-01],
       [8.4002483e-01, 9.2273450e-01, 5.6649190e-01, 7.5385606e-01,
        9.1587120e-01, 3.9596200e-02, 8.9435184e-01, 5.6475997e-01,
        2.3288828e-01, 1.9652534e-01],
       [2.1682233e-01, 2.8950059e-01, 7.5310403e-01, 6.7648250e-01,
        5.1056564e-02, 1.6518539e-01, 5.8806950e-01, 9.4541669e-02,
        6.3110876e-01, 2.9048622e-01],
       [5.7742238e-02, 3.1502587e-01, 5.6935811e-01, 2.2748303e-01,
        4.8667991e-01, 6.4949030e-01, 6.1752105e-01, 3.9268762e-01,
        2.7897447e-01, 5.5806071e-01]], dtype=float32)
</code></pre><h1 id="æ„å»ºè‡ªå®šä¹‰è¾¹æƒé‡çš„GNN"><a href="#æ„å»ºè‡ªå®šä¹‰è¾¹æƒé‡çš„GNN" class="headerlink" title="æ„å»ºè‡ªå®šä¹‰è¾¹æƒé‡çš„GNN"></a>æ„å»ºè‡ªå®šä¹‰è¾¹æƒé‡çš„GNN</h1><p>èƒ¡ä¹±å®šå€¼çš„<br>ä»£ç ä¿®æ”¹è‡ªï¼š<a href="https://zhuanlan.zhihu.com/p/426907570">https://zhuanlan.zhihu.com/p/426907570</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(data.num_features, <span class="number">16</span>, cached=<span class="literal">True</span>,</span><br><span class="line">                             normalize=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">#self.conv2 = GCNConv(16, data.num_classes, cached=True,</span></span><br><span class="line">        self.conv2 = GCNConv(<span class="number">16</span>, <span class="number">2</span>, cached=<span class="literal">True</span>,</span><br><span class="line">                             normalize=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># self.conv1 = ChebConv(data.num_features, 16, K=2)</span></span><br><span class="line">        <span class="comment"># self.conv2 = ChebConv(16, data.num_features, K=2)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr</span><br><span class="line">        x = F.relu(self.conv1(x, edge_index, edge_weight))</span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        x = self.conv2(x, edge_index, edge_weight)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv, ChebConv  <span class="comment"># noqa</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> data <span class="keyword">as</span> D</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>],[<span class="number">3</span>],[<span class="number">4</span>],[<span class="number">5</span>]], dtype=torch.<span class="built_in">float</span>)   <span class="comment"># N x emb(in)</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">x = torch.ones(<span class="number">10</span>,dtype=torch.<span class="built_in">float</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">y = torch.randint(<span class="number">0</span>,<span class="number">2</span>,[<span class="number">10</span>])</span><br><span class="line">train_mask = torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">True</span>], dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">val_mask=train_mask</span><br><span class="line">test_mask=train_mask</span><br><span class="line">data=D.Data()</span><br><span class="line">data.x,data.y,data.edge_index,data.edge_attr,data.train_mask,data.val_mask,data.test_mask \</span><br><span class="line">    = x,y,edge_index,edge_attr,train_mask,val_mask,test_mask</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 1])
torch.Size([10, 1])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model, data = Net().to(device), data.to(device)</span><br><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">    <span class="built_in">dict</span>(params=model.conv1.parameters(), weight_decay=<span class="number">5e-4</span>),</span><br><span class="line">    <span class="built_in">dict</span>(params=model.conv2.parameters(), weight_decay=<span class="number">0</span>)</span><br><span class="line">], lr=<span class="number">0.01</span>)  <span class="comment"># Only perform weight-decay on first convolution.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()</span><br><span class="line">    <span class="comment">#F.nll_loss(model()[data], data.y).backward()</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    logits, accs = model(), []</span><br><span class="line">    <span class="keyword">for</span> _, mask <span class="keyword">in</span> data(<span class="string">&#x27;train_mask&#x27;</span>, <span class="string">&#x27;val_mask&#x27;</span>, <span class="string">&#x27;test_mask&#x27;</span>):</span><br><span class="line">        pred = logits[mask].<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        acc = pred.eq(data.y[mask]).<span class="built_in">sum</span>().item() / mask.<span class="built_in">sum</span>().item()</span><br><span class="line">        accs.append(acc)</span><br><span class="line">    <span class="keyword">return</span> accs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_val_acc = test_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    train()</span><br><span class="line">    train_acc, val_acc, tmp_test_acc = test()</span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; best_val_acc:</span><br><span class="line">        best_val_acc = val_acc</span><br><span class="line">        test_acc = tmp_test_acc</span><br><span class="line">    log = <span class="string">&#x27;Epoch: &#123;:03d&#125;, Train: &#123;:.4f&#125;, Val: &#123;:.4f&#125;, Test: &#123;:.4f&#125;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(log.<span class="built_in">format</span>(epoch, train_acc, best_val_acc, test_acc))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 001, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 002, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 003, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 004, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 005, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 006, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 007, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 008, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 009, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 010, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 011, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 012, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 013, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 014, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 015, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 016, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 017, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 018, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 019, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 020, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 021, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 022, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 023, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 024, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 025, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 026, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 027, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 028, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 029, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 030, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 031, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 032, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 033, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 034, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 035, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 036, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 037, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 038, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 039, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 040, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 041, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 042, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 043, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 044, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 045, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 046, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 047, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 048, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 049, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 050, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 051, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 052, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 053, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 054, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 055, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 056, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 057, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 058, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 059, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 060, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 061, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 062, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 063, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 064, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 065, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 066, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 067, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 068, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 069, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 070, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 071, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 072, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 073, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 074, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 075, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 076, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 077, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 078, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 079, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 080, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 081, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 082, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 083, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 084, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 085, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 086, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 087, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 088, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 089, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 090, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 091, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 092, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 093, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 094, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 095, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 096, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 097, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 098, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 099, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 100, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 101, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 102, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 103, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 104, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 105, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 106, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 107, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 108, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 109, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 110, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 111, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 112, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 113, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 114, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 115, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 116, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 117, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 118, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 119, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 120, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 121, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 122, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 123, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 124, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 125, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 126, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 127, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 128, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 129, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 130, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 131, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 132, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 133, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 134, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 135, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 136, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 137, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 138, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 139, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 140, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 141, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 142, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 143, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 144, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 145, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 146, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 147, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 148, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 149, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 150, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 151, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 152, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 153, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 154, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 155, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 156, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 157, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 158, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 159, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 160, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 161, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 162, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 163, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 164, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 165, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 166, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 167, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 168, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 169, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 170, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 171, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 172, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 173, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 174, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 175, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 176, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 177, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 178, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 179, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 180, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 181, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 182, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 183, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 184, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 185, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 186, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 187, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 188, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 189, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 190, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 191, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 192, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 193, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 194, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 195, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 196, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 197, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 198, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 199, Train: 0.8000, Val: 0.8000, Test: 0.8000
Epoch: 200, Train: 0.8000, Val: 0.8000, Test: 0.8000
</code></pre>]]></content>
      <tags>
        <tag>PyG</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
</search>
